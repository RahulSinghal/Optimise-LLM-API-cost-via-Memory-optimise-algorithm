{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e75e81",
   "metadata": {},
   "source": [
    "# Mem0 Implementation: Scalable Long-Term Memory for LLM Agents (Refined Output)\n",
    "\n",
    "This notebook provides a detailed walkthrough and simplified implementation of the Mem0 architecture, focusing on its core mechanisms and comparing its token efficiency against a full-context approach. This version aims for more concise output during runs, storing detailed logs in variables for later inspection, and presenting the final token comparison in a structured DataFrame.\n",
    "\n",
    "**Scenario:** AI assistant helping a user plan a 'New Marketing Campaign'.\n",
    "\n",
    "**Core Mem0 Concepts Implemented (Simplified):**\n",
    "1.  Memory Extraction (`Ï•`)\n",
    "2.  Memory Storage (Textual facts with embeddings)\n",
    "3.  Memory Update Logic (ADD, UPDATE, NOOP via LLM)\n",
    "4.  Memory Retrieval (Semantic search for query context)\n",
    "\n",
    "**Improvements in this version:**\n",
    "- Reduced default console output during runs.\n",
    "- `VERBOSE_RAW_RUN` and `VERBOSE_MEM0_RUN` flags to enable detailed logging if needed.\n",
    "- Detailed turn-by-turn logs stored in `raw_run_log` and `mem0_run_log` variables.\n",
    "- Final token comparison presented as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ddc033",
   "metadata": {},
   "source": [
    "## 1. Setup: Libraries, API Configuration, and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb5054",
   "metadata": {},
   "source": [
    "### 1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa79ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to install the required packages:\n",
    "# !pip install openai scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2973a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required standard and third-party libraries for memory, LLM, and data handling\n",
    "import os      # OS interactions\n",
    "import json    # For JSON parsing/generation (LLM outputs)\n",
    "import time    # For delays between API calls\n",
    "import uuid    # For unique memory item IDs\n",
    "from datetime import datetime  # For timestamps\n",
    "\n",
    "import numpy as np            # For embedding vectors\n",
    "import pandas as pd           # For DataFrame/tabular analysis\n",
    "from openai import OpenAI     # For OpenAI-compatible LLM/embedding API\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # For embedding similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2832e",
   "metadata": {},
   "source": [
    "### 1.2. API and Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client configured. Using LLM: deepseek-ai/DeepSeek-V3, Embeddings: BAAI/bge-multilingual-gemma2\n"
     ]
    }
   ],
   "source": [
    "# --- API and Model Configuration ---\n",
    "\n",
    "# Get API key from environment variable (already set in variable API_KEY)\n",
    "# If not set, raise an error to prevent accidental unauthenticated requests\n",
    "API_KEY = os.getenv(\"NEBIUS_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API key not set. Please set the NEBIUS_API_KEY environment variable.\")\n",
    "\n",
    "# Set the base URL for the Nebius API\n",
    "BASE_URL = \"https://api.studio.nebius.com/v1/\"\n",
    "\n",
    "# Specify the LLM model to use for generating responses, extracting facts, and making update decisions\n",
    "# Using a more capable model is recommended for better Mem0 performance\n",
    "LLM_MODEL = \"deepseek-ai/DeepSeek-V3\"\n",
    "\n",
    "# Specify the embedding model to use for generating text embeddings\n",
    "EMBEDDING_MODEL = \"BAAI/bge-multilingual-gemma2\"\n",
    "\n",
    "# Initialize the OpenAI-compatible client with the specified base URL and API key\n",
    "client = OpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "print(f\"OpenAI client configured. Using LLM: {LLM_MODEL}, Embeddings: {EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb95122",
   "metadata": {},
   "source": [
    "### 1.3. Global Token Counters and Logging Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598086a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Token Counters ---\n",
    "# Track token usage for both approaches and each Mem0 sub-task\n",
    "total_prompt_tokens_raw, total_completion_tokens_raw = 0, 0  # Raw/full-context approach\n",
    "total_prompt_tokens_mem0_conversation, total_completion_tokens_mem0_conversation = 0, 0  # Mem0: conversational queries\n",
    "total_prompt_tokens_mem0_extraction, total_completion_tokens_mem0_extraction = 0, 0      # Mem0: extraction sub-task\n",
    "total_prompt_tokens_mem0_update, total_completion_tokens_mem0_update = 0, 0              # Mem0: update sub-task\n",
    "\n",
    "# --- Logging Variables ---\n",
    "# Store detailed logs for each run for later inspection or analysis\n",
    "raw_run_log = []   # Logs for the raw/full-context approach\n",
    "mem0_run_log = []  # Logs for the Mem0 approach\n",
    "\n",
    "def reset_all_token_counters_and_logs():\n",
    "       \"\"\"\n",
    "       Resets ALL global token counters and log lists to zero/empty.\n",
    "       This ensures a clean slate before running a new experiment or comparison.\n",
    "       \"\"\"\n",
    "       global total_prompt_tokens_raw, total_completion_tokens_raw, \\\n",
    "                 total_prompt_tokens_mem0_conversation, total_completion_tokens_mem0_conversation, \\\n",
    "                 total_prompt_tokens_mem0_extraction, total_completion_tokens_mem0_extraction, \\\n",
    "                 total_prompt_tokens_mem0_update, total_completion_tokens_mem0_update, \\\n",
    "                 raw_run_log, mem0_run_log\n",
    "\n",
    "       # Reset all token counters to zero\n",
    "       total_prompt_tokens_raw, total_completion_tokens_raw = 0, 0\n",
    "       total_prompt_tokens_mem0_conversation, total_completion_tokens_mem0_conversation = 0, 0\n",
    "       total_prompt_tokens_mem0_extraction, total_completion_tokens_mem0_extraction = 0, 0\n",
    "       total_prompt_tokens_mem0_update, total_completion_tokens_mem0_update = 0, 0\n",
    "\n",
    "       # Clear all run logs\n",
    "       raw_run_log = []\n",
    "       mem0_run_log = []\n",
    "       print(\"[Counters & Logs] All token counters and run logs have been reset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7c30b",
   "metadata": {},
   "source": [
    "### 1.4. Core LLM and Embedding Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5387e05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions for embeddings and LLM calls defined.\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text_to_embed, verbose=False):\n",
    "    \"\"\"\n",
    "    Generates an embedding vector for the given text using the configured embedding model.\n",
    "    Args:\n",
    "        text_to_embed (str): The input text to embed.\n",
    "        verbose (bool): If True, prints error messages.\n",
    "    Returns:\n",
    "        np.ndarray: Embedding vector as a numpy array. Returns a zero vector on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Call the embedding API to get the embedding vector\n",
    "        response = client.embeddings.create(model=EMBEDDING_MODEL, input=text_to_embed)\n",
    "        return np.array(response.data[0].embedding)\n",
    "    except Exception as e:\n",
    "        # On failure, print error (if verbose) and return a zero vector of expected dimension\n",
    "        if verbose: print(f\"[Error] Embedding failed for '{text_to_embed[:50]}...': {e}. Returning zero vector.\")\n",
    "        default_embedding_dim = 2560  # Adjust if your model's dimension is different\n",
    "        return np.zeros(default_embedding_dim)\n",
    "\n",
    "def get_llm_chat_completion(messages, temperature=0.1, max_tokens=150, verbose=False):\n",
    "    \"\"\"\n",
    "    Calls the LLM chat completion API with the given messages and parameters.\n",
    "    Args:\n",
    "        messages (list): List of message dicts for the chat history.\n",
    "        temperature (float): Sampling temperature for the LLM.\n",
    "        max_tokens (int): Maximum tokens to generate in the completion.\n",
    "        verbose (bool): If True, prints error messages.\n",
    "    Returns:\n",
    "        tuple: (response_content, prompt_tokens, completion_tokens)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Call the LLM chat completion API\n",
    "        response = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        prompt_tokens = response.usage.prompt_tokens if response.usage else 0\n",
    "        completion_tokens = response.usage.completion_tokens if response.usage else 0\n",
    "        return content, prompt_tokens, completion_tokens\n",
    "    except Exception as e:\n",
    "        # On failure, print error (if verbose) and return error message with zero tokens\n",
    "        if verbose: print(f\"[Error] LLM chat completion failed: {e}. Returning error message.\")\n",
    "        return f\"Error: LLM call failed. {e}\", 0, 0\n",
    "\n",
    "print(\"Helper functions for embeddings and LLM calls defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936cea58",
   "metadata": {},
   "source": [
    "## 2. The Conversational Scenario: Planning a Marketing Campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e13e009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversational scenario defined with 10 turns.\n"
     ]
    }
   ],
   "source": [
    "# Define the conversation script as a list of user turns (each turn is a dictionary)\n",
    "conversation_script = [\n",
    "    # User states the main goal for the campaign\n",
    "    {\"role\": \"user\", \"content\": \"Hi, let's start planning the 'New Marketing Campaign'. My primary goal is to increase brand awareness by 20%.\"},\n",
    "    # User specifies the target audience\n",
    "    {\"role\": \"user\", \"content\": \"For this campaign, the target audience is young adults aged 18-25.\"},\n",
    "    # User allocates an initial budget for social media ads\n",
    "    {\"role\": \"user\", \"content\": \"I want to allocate a budget of $5000 for social media ads for the New Marketing Campaign.\"},\n",
    "    # User asks about the main goal for the campaign\n",
    "    {\"role\": \"user\", \"content\": \"What's the main goal for the New Marketing Campaign?\"},\n",
    "    # User asks about the target audience\n",
    "    {\"role\": \"user\", \"content\": \"Who are we targeting for this campaign?\"},\n",
    "    # User adds a new task related to influencer research\n",
    "    {\"role\": \"user\", \"content\": \"Let's also consider influencers. Add a task: 'Research potential influencers for the 18-25 demographic' for the New Marketing Campaign.\"},\n",
    "    # User updates the budget for social media ads\n",
    "    {\"role\": \"user\", \"content\": \"Actually, let's increase the social media ad budget for the New Marketing Campaign to $7500.\"},\n",
    "    # User asks about the current budget for social media ads\n",
    "    {\"role\": \"user\", \"content\": \"What's the current budget for social media ads for the New Marketing Campaign?\"},\n",
    "    # User asks about pending tasks for the campaign\n",
    "    {\"role\": \"user\", \"content\": \"What tasks do I have pending for this campaign?\"},\n",
    "    # User expresses a preference for visual content\n",
    "    {\"role\": \"user\", \"content\": \"Also, for the New Marketing Campaign, I prefer visual content for this demographic, like short videos and infographics.\"}\n",
    "]\n",
    "\n",
    "print(f\"Conversational scenario defined with {len(conversation_script)} turns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7349b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_input(user_input):\n",
    "    \"\"\"\n",
    "    Classifies user input as a query or statement using the LLM.\n",
    "\n",
    "    Args:\n",
    "        user_input (str): The user's input message.\n",
    "\n",
    "    Returns:\n",
    "        str: \"query\" if the input is a question, \"statement\" otherwise.\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the LLM on classification rules\n",
    "    system_prompt = (\n",
    "        \"You are a classifier. \"\n",
    "        \"A 'query' is a question or request for information. \"\n",
    "        \"A 'statement' is a declaration, instruction, or information that is not a question. \"\n",
    "        \"Respond with only one word: either 'query' or 'statement'.\"\n",
    "    )\n",
    "    # Call the LLM to classify the input\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"microsoft/phi-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Classify the following input as a query or statement: {user_input}\"}\n",
    "        ]\n",
    "    )\n",
    "    # Extract and normalize the classification result\n",
    "    classification = response.choices[0].message.content.strip().lower()\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9db92df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Hi, let's start planning the 'New Marketing Campaign'. My primary goal is to increase brand awareness by 20%., Classification: statement\n",
      "Input: For this campaign, the target audience is young adults aged 18-25., Classification: statement\n",
      "Input: I want to allocate a budget of $5000 for social media ads for the New Marketing Campaign., Classification: statement\n",
      "Input: What's the main goal for the New Marketing Campaign?, Classification: query\n",
      "Input: Who are we targeting for this campaign?, Classification: query\n",
      "Input: Let's also consider influencers. Add a task: 'Research potential influencers for the 18-25 demographic' for the New Marketing Campaign., Classification: statement\n",
      "Input: Actually, let's increase the social media ad budget for the New Marketing Campaign to $7500., Classification: statement\n",
      "Input: What's the current budget for social media ads for the New Marketing Campaign?, Classification: query\n",
      "Input: What tasks do I have pending for this campaign?, Classification: query\n",
      "Input: Also, for the New Marketing Campaign, I prefer visual content for this demographic, like short videos and infographics., Classification: statement\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each turn in the conversation script\n",
    "for turn in conversation_script:\n",
    "    # Use the LLM to classify the user's input as either a 'query' or 'statement'\n",
    "    classification = classify_input(turn[\"content\"])\n",
    "    # Add the classification result to the turn dictionary under the 'type' key\n",
    "    turn[\"type\"] = classification\n",
    "    # Print the input and its classification for verification\n",
    "    print(f\"Input: {turn['content']}, Classification: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087d94c",
   "metadata": {},
   "source": [
    "## 3. Approach 1: Raw / Full-Context Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41db7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE_RAW_RUN = False  # Set to True for detailed turn-by-turn console output\n",
    "\n",
    "def run_raw_full_context_approach(script):\n",
    "    \"\"\"\n",
    "    Runs the conversation using the raw/full-context approach:\n",
    "    - Each user turn is appended to the full conversation history.\n",
    "    - The LLM receives the entire conversation history for every response.\n",
    "    - Token usage and responses are logged for analysis.\n",
    "    \"\"\"\n",
    "    global total_prompt_tokens_raw, total_completion_tokens_raw, raw_run_log\n",
    "\n",
    "    print(\"--- Running Raw/Full-Context Approach ---\")\n",
    "    # Initialize conversation history with a system prompt\n",
    "    current_conversation_history_for_llm = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Respond based on the full conversation history.\"}\n",
    "    ]\n",
    "\n",
    "    for i, turn_details in enumerate(script):\n",
    "        user_message_content = turn_details['content']\n",
    "        turn_type = turn_details['type']\n",
    "\n",
    "        if VERBOSE_RAW_RUN:\n",
    "            print(f\"\\n--- Raw Turn {i+1}/{len(script)} (Type: {turn_type}) ---\")\n",
    "        print(f\"Raw Turn {i+1} User: {user_message_content[:80]}...\")\n",
    "\n",
    "        # Add user message to conversation history\n",
    "        current_conversation_history_for_llm.append({\"role\": \"user\", \"content\": user_message_content})\n",
    "\n",
    "        # Call LLM with the full conversation history\n",
    "        assistant_response_text, p_tokens, c_tokens = get_llm_chat_completion(\n",
    "            current_conversation_history_for_llm, max_tokens=150, verbose=VERBOSE_RAW_RUN\n",
    "        )\n",
    "\n",
    "        # Update global token counters\n",
    "        total_prompt_tokens_raw += p_tokens\n",
    "        total_completion_tokens_raw += c_tokens\n",
    "\n",
    "        print(f\"Raw Turn {i+1} Assistant: {assistant_response_text[:80]}...\")\n",
    "        if VERBOSE_RAW_RUN:\n",
    "            print(f\"Tokens for this turn (Prompt: {p_tokens}, Completion: {c_tokens})\")\n",
    "            print(f\"Cumulative Raw Tokens (Prompt: {total_prompt_tokens_raw}, Completion: {total_completion_tokens_raw})\")\n",
    "\n",
    "        # Add assistant response to conversation history\n",
    "        current_conversation_history_for_llm.append({\"role\": \"assistant\", \"content\": assistant_response_text})\n",
    "\n",
    "        # Log turn details for later analysis\n",
    "        raw_run_log.append({\n",
    "            \"turn\": i + 1,\n",
    "            \"type\": turn_type,\n",
    "            \"user_content\": user_message_content,\n",
    "            \"assistant_response\": assistant_response_text,\n",
    "            \"prompt_tokens_turn\": p_tokens,\n",
    "            \"completion_tokens_turn\": c_tokens,\n",
    "            \"cumulative_prompt_tokens\": total_prompt_tokens_raw,\n",
    "            \"cumulative_completion_tokens\": total_completion_tokens_raw\n",
    "        })\n",
    "\n",
    "        time.sleep(0.2)  # Reduced sleep time, adjust if rate limits are an issue\n",
    "\n",
    "    print(\"\\n--- Raw/Full-Context Approach Summary ---\")\n",
    "    print(f\"Total Prompt Tokens: {total_prompt_tokens_raw}\")\n",
    "    print(f\"Total Completion Tokens: {total_completion_tokens_raw}\")\n",
    "    print(f\"Overall Total Tokens for Raw Approach: {total_prompt_tokens_raw + total_completion_tokens_raw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f70e41",
   "metadata": {},
   "source": [
    "## 4. Approach 2: Mem0 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1712c2",
   "metadata": {},
   "source": [
    "### 4.1. Memory Item and Memory Store Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c32e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryItem:\n",
    "    def __init__(self, text_content, source_turn_indices_list, verbose_embedding=False):\n",
    "        \"\"\"\n",
    "        Represents a single memory item with text, embedding, and metadata.\n",
    "\n",
    "        Args:\n",
    "            text_content (str): The content to store in memory.\n",
    "            source_turn_indices_list (list): List of conversation turn indices that contributed to this memory.\n",
    "            verbose_embedding (bool): If True, prints embedding errors.\n",
    "        \"\"\"\n",
    "        self.id = str(uuid.uuid4())  # Unique identifier for the memory item\n",
    "        self.text = text_content  # The actual memory content\n",
    "        self.embedding = get_embedding(text_content, verbose=verbose_embedding)  # Embedding vector\n",
    "        self.creation_timestamp = datetime.now()  # When the memory was created\n",
    "        self.last_accessed_timestamp = self.creation_timestamp  # Last time accessed\n",
    "        self.access_count = 0  # Number of times accessed\n",
    "        self.source_turn_indices = list(source_turn_indices_list)  # Source turns for provenance\n",
    "\n",
    "    def __repr__(self):\n",
    "        # String representation for debugging\n",
    "        return (f\"MemoryItem(id={self.id}, text='{self.text[:60]}...', \"\n",
    "                f\"created={self.creation_timestamp.strftime('%H:%M:%S')}, accessed={self.access_count})\")\n",
    "\n",
    "    def mark_accessed(self):\n",
    "        # Update access metadata when memory is accessed\n",
    "        self.last_accessed_timestamp = datetime.now()\n",
    "        self.access_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72a5cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryStore:\n",
    "    def __init__(self, verbose_ops=False):\n",
    "        \"\"\"\n",
    "        Stores and manages multiple MemoryItem instances.\n",
    "\n",
    "        Args:\n",
    "            verbose_ops (bool): If True, prints debug information for operations.\n",
    "        \"\"\"\n",
    "        self.memories = {}  # Dictionary to store memory_id -> MemoryItem\n",
    "        self.verbose = verbose_ops\n",
    "        if self.verbose:\n",
    "            print(\"[MemoryStore] Initialized an empty memory store.\")\n",
    "\n",
    "    def add_memory_item(self, memory_item_instance):\n",
    "        \"\"\"\n",
    "        Adds a new MemoryItem to the store.\n",
    "\n",
    "        Args:\n",
    "            memory_item_instance (MemoryItem): The memory item to add.\n",
    "        \"\"\"\n",
    "        self.memories[memory_item_instance.id] = memory_item_instance\n",
    "        if self.verbose:\n",
    "            print(f\"[MemoryStore] ADDED: '{memory_item_instance.text[:70]}' (ID: {memory_item_instance.id})\")\n",
    "\n",
    "    def get_memory_item_by_id(self, memory_id):\n",
    "        \"\"\"\n",
    "        Retrieves a MemoryItem by its ID and marks it as accessed.\n",
    "\n",
    "        Args:\n",
    "            memory_id (str): The ID of the memory item.\n",
    "\n",
    "        Returns:\n",
    "            MemoryItem or None: The memory item if found, else None.\n",
    "        \"\"\"\n",
    "        if memory_id in self.memories:\n",
    "            self.memories[memory_id].mark_accessed()  # Mark as accessed\n",
    "            return self.memories[memory_id]\n",
    "        return None\n",
    "\n",
    "    def update_existing_memory_item(self, memory_id, new_text_content, contributing_turn_indices):\n",
    "        \"\"\"\n",
    "        Updates the text and embedding of an existing memory item.\n",
    "\n",
    "        Args:\n",
    "            memory_id (str): The ID of the memory to update.\n",
    "            new_text_content (str): The new text content.\n",
    "            contributing_turn_indices (list): Additional turn indices to add.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if update succeeded, False otherwise.\n",
    "        \"\"\"\n",
    "        if memory_id in self.memories:\n",
    "            memory_to_update = self.memories[memory_id]\n",
    "            original_text_preview = memory_to_update.text[:70]\n",
    "            # Update text and embedding\n",
    "            memory_to_update.text = new_text_content\n",
    "            memory_to_update.embedding = get_embedding(new_text_content, verbose=self.verbose)\n",
    "            memory_to_update.creation_timestamp = datetime.now()\n",
    "            # Add new contributing turn indices if not already present\n",
    "            for turn_idx in contributing_turn_indices:\n",
    "                if turn_idx not in memory_to_update.source_turn_indices:\n",
    "                    memory_to_update.source_turn_indices.append(turn_idx)\n",
    "            memory_to_update.mark_accessed()\n",
    "            if self.verbose:\n",
    "                print(f\"[MemoryStore] UPDATED ID {memory_id}: From '{original_text_preview}' TO '{new_text_content[:70]}'\")\n",
    "            return True\n",
    "        if self.verbose:\n",
    "            print(f\"[MemoryStore] UPDATE FAILED: ID {memory_id} not found.\")\n",
    "        return False\n",
    "\n",
    "    def find_semantically_similar_memories(self, query_text_embedding, top_s_results=3, similarity_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Finds memories most similar to a query embedding.\n",
    "\n",
    "        Args:\n",
    "            query_text_embedding (np.ndarray): Embedding of the query text.\n",
    "            top_s_results (int): Max number of results to return.\n",
    "            similarity_threshold (float): Minimum similarity score to consider.\n",
    "\n",
    "        Returns:\n",
    "            list: Tuples of (MemoryItem, similarity_score).\n",
    "        \"\"\"\n",
    "        # Return empty if no memories or invalid query embedding\n",
    "        if not self.memories or query_text_embedding is None or query_text_embedding.size == 0:\n",
    "            return []\n",
    "        # Filter out memories with invalid embeddings\n",
    "        valid_memories_for_similarity = [\n",
    "            (mid, self.memories[mid].embedding) for mid in self.memories\n",
    "            if self.memories[mid].embedding is not None and \n",
    "               self.memories[mid].embedding.size > 0 and \n",
    "               np.any(self.memories[mid].embedding)  # Ensure not all zeros\n",
    "        ]\n",
    "        if not valid_memories_for_similarity:\n",
    "            return []\n",
    "        valid_memory_ids = [item[0] for item in valid_memories_for_similarity]\n",
    "        valid_memory_embeddings = np.array([item[1] for item in valid_memories_for_similarity])\n",
    "        # Handle case of single valid memory\n",
    "        if valid_memory_embeddings.ndim == 1:\n",
    "            valid_memory_embeddings = valid_memory_embeddings.reshape(1, -1)\n",
    "        if query_text_embedding.ndim == 1:\n",
    "            query_text_embedding = query_text_embedding.reshape(1, -1)\n",
    "\n",
    "        # Compute cosine similarity between query and all valid memories\n",
    "        similarities_vector = cosine_similarity(query_text_embedding, valid_memory_embeddings)[0]\n",
    "        sorted_similarity_indices = np.argsort(similarities_vector)[::-1]\n",
    "        retrieved_similar_memories = []\n",
    "        # Collect top results above threshold\n",
    "        for i in range(min(top_s_results, len(sorted_similarity_indices))):\n",
    "            idx = sorted_similarity_indices[i]\n",
    "            similarity_score = similarities_vector[idx]\n",
    "            if similarity_score >= similarity_threshold:\n",
    "                retrieved_similar_memories.append((self.memories[valid_memory_ids[idx]], similarity_score))\n",
    "            else:\n",
    "                break\n",
    "        return retrieved_similar_memories\n",
    "\n",
    "    def clear_store(self):\n",
    "        \"\"\"\n",
    "        Clears all memories from the store.\n",
    "        \"\"\"\n",
    "        self.memories = {}\n",
    "        if self.verbose:\n",
    "            print(\"[MemoryStore] Memory store has been cleared.\")\n",
    "\n",
    "# Instantiate the memory store (set verbose_ops=True for debug output)\n",
    "mem0_memory_store = MemoryStore(verbose_ops=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb20d58a",
   "metadata": {},
   "source": [
    "### 4.2. Memory Extraction Function (`Ï•`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bdce8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem0_extract_salient_facts_from_turn(current_user_statement_text, recent_turns_window_text, current_turn_index_in_script, verbose=False):\n",
    "    \"\"\"\n",
    "    Uses the LLM to extract concise, self-contained, declarative facts from a user's statement,\n",
    "    considering recent conversation context. Returns a list of fact strings.\n",
    "\n",
    "    Args:\n",
    "        current_user_statement_text (str): The user's current statement.\n",
    "        recent_turns_window_text (str): Recent conversation context as text.\n",
    "        current_turn_index_in_script (int): Index of the current turn in the script.\n",
    "        verbose (bool): If True, prints debug information.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted fact strings.\n",
    "    \"\"\"\n",
    "    global total_prompt_tokens_mem0_extraction, total_completion_tokens_mem0_extraction\n",
    "\n",
    "    # Build the extraction prompt for the LLM\n",
    "    extraction_prompt_template = f\"\"\"\n",
    "    You are an AI expert in extracting key information from dialogue.\n",
    "    Analyze 'New User Statement' in context of 'Recent Conversation Context'.\n",
    "    Extract concise, self-contained, declarative facts representing new, important user-provided information from 'New User Statement'.\n",
    "    Each fact: complete sentence. Focus: user's goals, plans, preferences, decisions, key entities.\n",
    "    Avoid: questions, acknowledgements, fluff. If statement updates info, extract updated fact. Do NOT infer.\n",
    "    Recent Conversation Context:\n",
    "    ---BEGIN CONTEXT---\n",
    "    {recent_turns_window_text if recent_turns_window_text else \"(No prior context)\"}\n",
    "    ---END CONTEXT---\n",
    "    New User Statement to Process: \"{current_user_statement_text}\"\n",
    "    Output ONLY a valid JSON list of strings (facts). E.g.: [\"Fact 1.\", \"Fact 2.\"]. Empty list [] if no new salient facts.\n",
    "    Extracted Facts (JSON list):\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare messages for the LLM call\n",
    "    extraction_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Expert extraction AI. Output ONLY valid JSON list of facts.\"},\n",
    "        {\"role\": \"user\", \"content\": extraction_prompt_template}\n",
    "    ]\n",
    "\n",
    "    # Call the LLM to extract facts\n",
    "    llm_extraction_response_text, p_tokens, c_tokens = get_llm_chat_completion(\n",
    "        extraction_messages, temperature=0.0, max_tokens=250, verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Update global token counters\n",
    "    total_prompt_tokens_mem0_extraction += p_tokens\n",
    "    total_completion_tokens_mem0_extraction += c_tokens\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[Extractor LLM] Raw Output: {llm_extraction_response_text}\")\n",
    "\n",
    "    # Attempt to parse the LLM's output as a JSON list of strings\n",
    "    try:\n",
    "        json_start_index = llm_extraction_response_text.find('[')\n",
    "        json_end_index = llm_extraction_response_text.rfind(']')\n",
    "        if json_start_index != -1 and json_end_index != -1 and json_end_index > json_start_index:\n",
    "            json_string_candidate = llm_extraction_response_text[json_start_index : json_end_index+1]\n",
    "            parsed_facts_list = json.loads(json_string_candidate)\n",
    "            if isinstance(parsed_facts_list, list) and all(isinstance(fact, str) for fact in parsed_facts_list):\n",
    "                if verbose or len(parsed_facts_list) > 0:\n",
    "                    print(f\"[Extractor LLM] Parsed {len(parsed_facts_list)} fact(s).\")\n",
    "                return parsed_facts_list\n",
    "            if verbose:\n",
    "                print(f\"[Extractor LLM] Warning: Parsed JSON not list of strings: {parsed_facts_list}. Returning [].\")\n",
    "        elif verbose:\n",
    "            print(f\"[Extractor LLM] Warning: No valid JSON list brackets in: '{llm_extraction_response_text}'. Returning [].\")\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"[Extractor LLM] Error parsing JSON: {e}. Response: '{llm_extraction_response_text}'. Returning [].\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f7331",
   "metadata": {},
   "source": [
    "### 4.3. Memory Update Logic (ADD, UPDATE, NOOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7cf5bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of similar memories to consider for update decision\n",
    "S_SIMILAR_MEMORIES_FOR_UPDATE_DECISION = 3\n",
    "\n",
    "def mem0_decide_memory_operation_with_llm(candidate_fact_text, similar_existing_memories_list, verbose=False):\n",
    "    \"\"\"\n",
    "    Uses the LLM to decide whether to ADD, UPDATE, or NOOP a candidate fact in memory,\n",
    "    based on its similarity to existing memories.\n",
    "\n",
    "    Args:\n",
    "        candidate_fact_text (str): The new fact extracted from user input.\n",
    "        similar_existing_memories_list (list): List of (MemoryItem, similarity_score) tuples.\n",
    "        verbose (bool): If True, prints debug information.\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON object with operation decision, e.g.,\n",
    "              {\"operation\": \"ADD\"} or\n",
    "              {\"operation\": \"UPDATE\", \"target_memory_id\": \"ID\", \"updated_memory_text\": \"Text\"} or\n",
    "              {\"operation\": \"NOOP\"}\n",
    "    \"\"\"\n",
    "    global total_prompt_tokens_mem0_update, total_completion_tokens_mem0_update\n",
    "\n",
    "    # Prepare prompt segment describing similar memories, if any\n",
    "    similar_memories_prompt_segment = \"No highly similar memories found.\"\n",
    "    if similar_existing_memories_list:\n",
    "        formatted_list = [\n",
    "            f\"  {i+1}. ID: {mem.id}, Sim: {sim_score:.4f}, Text: '{mem.text}'\"\n",
    "            for i, (mem, sim_score) in enumerate(similar_existing_memories_list)\n",
    "        ]\n",
    "        similar_memories_prompt_segment = \"Existing Similar Memories:\\n\" + \"\\n\".join(formatted_list)\n",
    "    \n",
    "    # Build the update decision prompt for the LLM\n",
    "    formatted_update_decision_prompt = f\"\"\"\n",
    "    AI Memory Consolidation Module.\n",
    "    Task: Integrate 'New Candidate Fact' into memory store.\n",
    "    New Candidate Fact: \"{candidate_fact_text}\"\n",
    "    {similar_memories_prompt_segment}\n",
    "    Decide ONE operation: ADD, UPDATE, or NOOP.\n",
    "    Rules:\n",
    "    1. ADD: If new info, not covered by similar memories (or no similar found).\n",
    "    2. UPDATE: If fact corrects, makes current, or adds essential detail to ONE similar memory, superseding it. Specify 'target_memory_id' (from list) and 'updated_memory_text' (usually New Candidate Fact text).\n",
    "    3. NOOP: If redundant or no new substantive value over existing similar memories.\n",
    "    Output STRICTLY JSON: {{ \"operation\": \"ADD\" }} OR {{ \"operation\": \"UPDATE\", \"target_memory_id\": \"ID\", \"updated_memory_text\": \"Text\" }} OR {{ \"operation\": \"NOOP\" }}\n",
    "    Decision (JSON object):\n",
    "    \"\"\"\n",
    "    # Prepare messages for the LLM call\n",
    "    update_decision_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Expert memory AI. Output ONLY valid JSON decision as instructed.\"},\n",
    "        {\"role\": \"user\", \"content\": formatted_update_decision_prompt}\n",
    "    ]\n",
    "\n",
    "    # Call the LLM to get the update decision\n",
    "    llm_decision_response_text, p_tokens, c_tokens = get_llm_chat_completion(\n",
    "        update_decision_messages, temperature=0.0, max_tokens=200, verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Update global token counters\n",
    "    total_prompt_tokens_mem0_update += p_tokens\n",
    "    total_completion_tokens_mem0_update += c_tokens\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[Updater LLM] Raw Output: {llm_decision_response_text}\")\n",
    "\n",
    "    # Attempt to parse the LLM's output as a JSON object\n",
    "    try:\n",
    "        json_start_index = llm_decision_response_text.find('{')\n",
    "        json_end_index = llm_decision_response_text.rfind('}')\n",
    "        if json_start_index != -1 and json_end_index != -1 and json_end_index > json_start_index:\n",
    "            json_string_candidate = llm_decision_response_text[json_start_index : json_end_index+1]\n",
    "            parsed_decision_json = json.loads(json_string_candidate)\n",
    "            op = parsed_decision_json.get(\"operation\")\n",
    "            # Check for valid operation types and required fields\n",
    "            if op in [\"ADD\", \"NOOP\"]:\n",
    "                if verbose or op == \"NOOP\":\n",
    "                    print(f\"[Updater LLM] Parsed decision: {op}\")\n",
    "                return parsed_decision_json\n",
    "            elif op == \"UPDATE\" and \"target_memory_id\" in parsed_decision_json and \"updated_memory_text\" in parsed_decision_json:\n",
    "                if verbose:\n",
    "                    print(f\"[Updater LLM] Parsed decision: {op}\")\n",
    "                return parsed_decision_json\n",
    "            if verbose:\n",
    "                print(f\"[Updater LLM] Warning: Invalid decision structure: {parsed_decision_json}. Defaulting to ADD.\")\n",
    "        elif verbose:\n",
    "            print(f\"[Updater LLM] Warning: No valid JSON object brackets in: '{llm_decision_response_text}'. Defaulting to ADD.\")\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"[Updater LLM] Error parsing JSON: {e}. Response: '{llm_decision_response_text}'. Defaulting to ADD.\")\n",
    "    # Default fallback: ADD operation\n",
    "    return {\"operation\": \"ADD\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b20f3",
   "metadata": {},
   "source": [
    "### 4.4. Orchestrating Memory Extraction and Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9fa5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem0_process_user_statement_for_memory(user_statement_text, recent_turns_context_text, memory_store_instance, current_turn_idx, turn_log_entry, verbose=False):\n",
    "    \"\"\"\n",
    "    Orchestrates the process of extracting salient facts from a user's statement,\n",
    "    determining memory operations (ADD, UPDATE, NOOP) for each fact, and updating the memory store accordingly.\n",
    "\n",
    "    Args:\n",
    "        user_statement_text (str): The user's statement to process.\n",
    "        recent_turns_context_text (str): Recent conversation context for extraction.\n",
    "        memory_store_instance (MemoryStore): The memory store to update.\n",
    "        current_turn_idx (int): Index of the current turn in the script.\n",
    "        turn_log_entry (dict): Log entry for this turn (for detailed logging).\n",
    "        verbose (bool): If True, prints detailed debug information.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"[MemoryOrchestrator] Processing: '{user_statement_text[:60]}...' (Turn: {current_turn_idx})\")\n",
    "    turn_log_entry['extraction_details'] = []\n",
    "    turn_log_entry['update_details'] = []\n",
    "\n",
    "    # Step 1: Extract candidate facts from the user statement using the LLM extractor\n",
    "    candidate_facts_list = mem0_extract_salient_facts_from_turn(\n",
    "        user_statement_text, recent_turns_context_text, current_turn_idx, verbose=verbose\n",
    "    )\n",
    "    turn_log_entry['extracted_facts_raw'] = list(candidate_facts_list)\n",
    "    if not candidate_facts_list:\n",
    "        if verbose:\n",
    "            print(\"[MemoryOrchestrator] No candidate facts extracted.\")\n",
    "        return\n",
    "    if verbose or len(candidate_facts_list) > 0:\n",
    "        print(f\"[MemoryOrchestrator] Extracted {len(candidate_facts_list)} fact(s).\")\n",
    "\n",
    "    # Step 2: For each extracted fact, determine the appropriate memory operation\n",
    "    for fact_idx, individual_fact_text in enumerate(candidate_facts_list):\n",
    "        extraction_detail = {\n",
    "            \"candidate_fact\": individual_fact_text,\n",
    "            \"similar_memories_checked\": [],\n",
    "            \"llm_decision\": None\n",
    "        }\n",
    "        if verbose:\n",
    "            print(f\"\\n[MemoryOrchestrator] -> Processing Fact {fact_idx+1}: '{individual_fact_text[:60]}...'\")\n",
    "        # Generate embedding for the candidate fact\n",
    "        candidate_fact_embedding = get_embedding(individual_fact_text, verbose=verbose)\n",
    "        # Find similar memories in the store\n",
    "        similar_memories_found = memory_store_instance.find_semantically_similar_memories(\n",
    "            candidate_fact_embedding, top_s_results=S_SIMILAR_MEMORIES_FOR_UPDATE_DECISION\n",
    "        )\n",
    "        if similar_memories_found:\n",
    "            if verbose:\n",
    "                print(f\"[MemoryOrchestrator]    Found {len(similar_memories_found)} similar memories.\")\n",
    "            for mem, sim_score in similar_memories_found:\n",
    "                extraction_detail['similar_memories_checked'].append({\n",
    "                    'id': mem.id,\n",
    "                    'text': mem.text,\n",
    "                    'similarity': sim_score\n",
    "                })\n",
    "                if verbose:\n",
    "                    print(f\"[MemoryOrchestrator]      ID: {mem.id}, Sim: {sim_score:.2f}, Text: '{mem.text[:50]}...'\")\n",
    "        elif verbose:\n",
    "            print(\"[MemoryOrchestrator]    No highly similar memories found.\")\n",
    "\n",
    "        # Step 3: Use LLM to decide on ADD, UPDATE, or NOOP operation\n",
    "        llm_decision_json = mem0_decide_memory_operation_with_llm(\n",
    "            individual_fact_text, similar_memories_found, verbose=verbose\n",
    "        )\n",
    "        operation_to_perform = llm_decision_json.get(\"operation\")\n",
    "        extraction_detail['llm_decision'] = llm_decision_json\n",
    "        print(f\"[MemoryOrchestrator] Fact '{individual_fact_text[:30]}...': LLM Decision -> {operation_to_perform}\")\n",
    "        turn_log_entry['update_details'].append(extraction_detail)  # Log before executing\n",
    "\n",
    "        # Step 4: Execute the decided memory operation\n",
    "        if operation_to_perform == \"ADD\":\n",
    "            # Add new memory item to the store\n",
    "            new_memory_item = MemoryItem(\n",
    "                text_content=individual_fact_text,\n",
    "                source_turn_indices_list=[current_turn_idx],\n",
    "                verbose_embedding=verbose\n",
    "            )\n",
    "            memory_store_instance.add_memory_item(new_memory_item)\n",
    "        elif operation_to_perform == \"UPDATE\":\n",
    "            # Update an existing memory item\n",
    "            target_memory_id = llm_decision_json.get(\"target_memory_id\")\n",
    "            updated_fact_text = llm_decision_json.get(\"updated_memory_text\")\n",
    "            # Check if the target ID is plausible (in similar memories)\n",
    "            is_plausible_target = any(mem.id == target_memory_id for mem, _ in similar_memories_found)\n",
    "            if target_memory_id and updated_fact_text and is_plausible_target:\n",
    "                memory_store_instance.update_existing_memory_item(\n",
    "                    target_memory_id, updated_fact_text, [current_turn_idx]\n",
    "                )\n",
    "            elif target_memory_id and updated_fact_text:\n",
    "                # Try to update even if not in similar list; fallback to add if fails\n",
    "                if verbose:\n",
    "                    print(f\"[MemoryOrchestrator] Warning: UPDATE target_id '{target_memory_id}' not in similar list. Attempting update.\")\n",
    "                if not memory_store_instance.update_existing_memory_item(\n",
    "                    target_memory_id, updated_fact_text, [current_turn_idx]\n",
    "                ):\n",
    "                    if verbose:\n",
    "                        print(f\"[MemoryOrchestrator] UPDATE failed. Adding as new.\")\n",
    "                    memory_store_instance.add_memory_item(MemoryItem(individual_fact_text, [current_turn_idx], verbose))\n",
    "            else:\n",
    "                # Malformed UPDATE, fallback to add as new\n",
    "                if verbose:\n",
    "                    print(f\"[MemoryOrchestrator] Warning: UPDATE malformed/implausible. Adding as new.\")\n",
    "                memory_store_instance.add_memory_item(MemoryItem(individual_fact_text, [current_turn_idx], verbose))\n",
    "        elif operation_to_perform == \"NOOP\":\n",
    "            # Do nothing for redundant facts\n",
    "            if verbose:\n",
    "                print(f\"[MemoryOrchestrator]    NOOP for fact: '{individual_fact_text[:70]}...'\")\n",
    "        else:\n",
    "            # Fallback: treat as ADD if unknown operation\n",
    "            if verbose:\n",
    "                print(f\"[MemoryOrchestrator] Warning: Unknown op '{operation_to_perform}'. Defaulting to ADD.\")\n",
    "            memory_store_instance.add_memory_item(MemoryItem(individual_fact_text, [current_turn_idx], verbose))\n",
    "        time.sleep(0.1)  # Small delay to avoid API rate limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322108c5",
   "metadata": {},
   "source": [
    "### 4.5. Memory Retrieval for Answering User Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c9c41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_MEMORIES_TO_RETRIEVE_FOR_QUERY = 3\n",
    "\n",
    "def mem0_retrieve_and_format_memories_for_llm_query(\n",
    "    user_query_text, memory_store_instance, turn_log_entry, \n",
    "    top_k_results=K_MEMORIES_TO_RETRIEVE_FOR_QUERY, verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k semantically relevant memories for a user query and formats them for LLM input.\n",
    "\n",
    "    Args:\n",
    "        user_query_text (str): The user's query.\n",
    "        memory_store_instance (MemoryStore): The memory store to search.\n",
    "        turn_log_entry (dict): Log entry for this turn (for detailed logging).\n",
    "        top_k_results (int): Number of top relevant memories to retrieve.\n",
    "        verbose (bool): If True, prints debug information.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted string of relevant memories for LLM, or a message if none found.\n",
    "    \"\"\"\n",
    "    # Initialize log for retrieved memories\n",
    "    turn_log_entry['retrieved_memories_for_query'] = []\n",
    "\n",
    "    # Return early if query is empty or memory store is empty\n",
    "    if not user_query_text.strip() or not memory_store_instance.memories:\n",
    "        return \"(No relevant memories in store or query empty.)\"\n",
    "        \n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = get_embedding(user_query_text, verbose=verbose)\n",
    "\n",
    "    # Retrieve top-k semantically similar memories\n",
    "    retrieved_memories_with_scores = memory_store_instance.find_semantically_similar_memories(\n",
    "        query_embedding, top_s_results=top_k_results\n",
    "    )\n",
    "\n",
    "    # If no relevant memories found, return message\n",
    "    if not retrieved_memories_with_scores:\n",
    "        return \"(No relevant memories found for this query.)\"\n",
    "    \n",
    "    # Format the retrieved memories for LLM input\n",
    "    formatted_memories_string = \"Based on my memory, here's relevant information:\\n\"\n",
    "    for i, (mem_item, similarity_score) in enumerate(retrieved_memories_with_scores):\n",
    "        memory_store_instance.get_memory_item_by_id(mem_item.id)  # Mark as accessed\n",
    "        formatted_memories_string += f\"  {i+1}. {mem_item.text} (Similarity: {similarity_score:.3f})\\n\"\n",
    "        # Log retrieved memory details\n",
    "        turn_log_entry['retrieved_memories_for_query'].append({\n",
    "            'id': mem_item.id, \n",
    "            'text': mem_item.text, \n",
    "            'similarity': similarity_score\n",
    "        })\n",
    "    if verbose:\n",
    "        print(f\"[Retriever] Formatted memories for LLM: \\n{formatted_memories_string}\")\n",
    "    return formatted_memories_string.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd055d",
   "metadata": {},
   "source": [
    "### 4.6. Main Function: Running the Mem0-Powered Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cb42453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controls verbosity for Mem0 run (set True for detailed output)\n",
    "VERBOSE_MEM0_RUN = False\n",
    "\n",
    "# Number of recent turns to use as context for memory extraction\n",
    "M_RECENT_RAW_TURNS_FOR_EXTRACTION_CONTEXT = 3\n",
    "\n",
    "# Number of recent turns to keep in short-term chat history for LLM context\n",
    "SHORT_TERM_CHAT_HISTORY_WINDOW = 2\n",
    "\n",
    "def run_mem0_approach_conversation(script, memory_store_instance):\n",
    "    \"\"\"\n",
    "    Runs a conversation using the Mem0 memory-powered approach.\n",
    "    Handles both user statements (for memory extraction/update) and queries (for memory retrieval/answering).\n",
    "    Tracks token usage and logs details for each turn.\n",
    "\n",
    "    Args:\n",
    "        script (list): List of dicts, each representing a user turn with 'content' and 'type'.\n",
    "        memory_store_instance (MemoryStore): The memory store to use for this run.\n",
    "    \"\"\"\n",
    "    global total_prompt_tokens_mem0_conversation, total_completion_tokens_mem0_conversation, mem0_run_log\n",
    "\n",
    "    # Reset memory store and set verbosity\n",
    "    memory_store_instance.clear_store()\n",
    "    memory_store_instance.verbose = VERBOSE_MEM0_RUN\n",
    "\n",
    "    print(\"--- Running Mem0-Powered Conversational Approach ---\")\n",
    "\n",
    "    # Raw log for extraction context (for fact extraction)\n",
    "    raw_conversation_log_for_extraction_context = []\n",
    "\n",
    "    # Short-term chat history for LLM context (system prompt + last N turns)\n",
    "    current_short_term_llm_chat_history = [\n",
    "        {\"role\": \"system\", \"content\": \"Helpful AI assistant. Use general knowledge and 'Relevant Information from Memory' to answer concisely.\"}\n",
    "    ]\n",
    "\n",
    "    for turn_index, turn_data in enumerate(script):\n",
    "        user_message_content = turn_data['content']\n",
    "        turn_type = turn_data['type']\n",
    "        turn_log_entry = {\n",
    "            \"turn\": turn_index + 1,\n",
    "            \"type\": turn_type,\n",
    "            \"user_content\": user_message_content\n",
    "        }\n",
    "\n",
    "        print(f\"\\n--- Mem0 Turn {turn_index + 1}/{len(script)} ({turn_type}) ---\")\n",
    "        print(f\"User: {user_message_content[:80]}...\")\n",
    "\n",
    "        assistant_response_text = \"(Ack/Internal Processing)\"\n",
    "\n",
    "        # Handle user statements (add/update memory)\n",
    "        if turn_type == 'statement' or turn_type == 'statement_update':\n",
    "            # Get recent turns for extraction context\n",
    "            start_idx = max(0, len(raw_conversation_log_for_extraction_context) - M_RECENT_RAW_TURNS_FOR_EXTRACTION_CONTEXT)\n",
    "            recent_turns_text = \"\\n\".join(raw_conversation_log_for_extraction_context[start_idx:])\n",
    "\n",
    "            # Extract facts and update memory store\n",
    "            mem0_process_user_statement_for_memory(\n",
    "                user_message_content,\n",
    "                recent_turns_text,\n",
    "                memory_store_instance,\n",
    "                turn_index,\n",
    "                turn_log_entry,\n",
    "                verbose=VERBOSE_MEM0_RUN\n",
    "            )\n",
    "\n",
    "            # Provide acknowledgement response\n",
    "            assistant_response_text = \"Okay, noted.\" if turn_type == 'statement' else \"Okay, updated.\"\n",
    "            print(f\"Assistant (Ack): {assistant_response_text}\")\n",
    "\n",
    "            # Log response and token usage (no LLM call for ack)\n",
    "            turn_log_entry['assistant_response_conversational'] = assistant_response_text\n",
    "            turn_log_entry['prompt_tokens_conversational_turn'] = 0\n",
    "            turn_log_entry['completion_tokens_conversational_turn'] = 0\n",
    "\n",
    "        # Handle user queries (retrieve memory and answer)\n",
    "        elif turn_type == 'query':\n",
    "            if VERBOSE_MEM0_RUN:\n",
    "                print(f\"[Mem0 Run] Processing query: '{user_message_content}'\")\n",
    "\n",
    "            # Retrieve relevant memories for the query\n",
    "            retrieved_memories_text = mem0_retrieve_and_format_memories_for_llm_query(\n",
    "                user_message_content,\n",
    "                memory_store_instance,\n",
    "                turn_log_entry,\n",
    "                verbose=VERBOSE_MEM0_RUN\n",
    "            )\n",
    "\n",
    "            # Prepare LLM input: short-term chat + user query + relevant memories\n",
    "            messages_for_llm = list(current_short_term_llm_chat_history)\n",
    "            messages_for_llm.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"User Query: '{user_message_content}'\\n\\nRelevant Info from Memory:\\n{retrieved_memories_text}\"\n",
    "            })\n",
    "\n",
    "            # Get assistant response from LLM\n",
    "            assistant_response_text, p_tokens, c_tokens = get_llm_chat_completion(\n",
    "                messages_for_llm,\n",
    "                max_tokens=120,\n",
    "                verbose=VERBOSE_MEM0_RUN\n",
    "            )\n",
    "\n",
    "            # Update global token counters\n",
    "            total_prompt_tokens_mem0_conversation += p_tokens\n",
    "            total_completion_tokens_mem0_conversation += c_tokens\n",
    "\n",
    "            print(f\"Assistant: {assistant_response_text[:80]}...\")\n",
    "            if VERBOSE_MEM0_RUN:\n",
    "                print(f\"  Tokens for query response (P: {p_tokens}, C: {c_tokens})\")\n",
    "\n",
    "            # Log response and token usage\n",
    "            turn_log_entry['assistant_response_conversational'] = assistant_response_text\n",
    "            turn_log_entry['prompt_tokens_conversational_turn'] = p_tokens\n",
    "            turn_log_entry['completion_tokens_conversational_turn'] = c_tokens\n",
    "\n",
    "        # Update extraction context log (for fact extraction in future turns)\n",
    "        raw_conversation_log_for_extraction_context.append(f\"T{turn_index+1} U: {user_message_content}\")\n",
    "        raw_conversation_log_for_extraction_context.append(f\"T{turn_index+1} A: {assistant_response_text}\")\n",
    "\n",
    "        # Update short-term chat history for LLM context\n",
    "        current_short_term_llm_chat_history.extend([\n",
    "            {\"role\": \"user\", \"content\": user_message_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_response_text}\n",
    "        ])\n",
    "        # Truncate chat history to keep only the most recent N turns (plus system prompt)\n",
    "        if len(current_short_term_llm_chat_history) > (1 + SHORT_TERM_CHAT_HISTORY_WINDOW * 2):\n",
    "            current_short_term_llm_chat_history = [current_short_term_llm_chat_history[0]] + \\\n",
    "                current_short_term_llm_chat_history[-(SHORT_TERM_CHAT_HISTORY_WINDOW*2):]\n",
    "\n",
    "        # Log memory store size after this turn\n",
    "        turn_log_entry['mem_store_size_after_turn'] = len(memory_store_instance.memories)\n",
    "        mem0_run_log.append(turn_log_entry)\n",
    "\n",
    "        if VERBOSE_MEM0_RUN:\n",
    "            print(f\"  Current Mem0 Store Size: {len(memory_store_instance.memories)}\")\n",
    "\n",
    "        # Small delay to avoid API rate limits\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    print(\"\\n--- Mem0-Powered Conversational Approach Summary (Global Counters) ---\")\n",
    "    # Final token totals are available via global counters if needed\n",
    "\n",
    "    if VERBOSE_MEM0_RUN:\n",
    "        print(\"\\n--- Final Content of Mem0 Memory Store ---\")\n",
    "        if memory_store_instance.memories:\n",
    "            for mem_id, mem_item in memory_store_instance.memories.items():\n",
    "                print(f\"  ID: {mem_id}\\n    Text: '{mem_item.text}'\\n    Created: {mem_item.creation_timestamp.strftime('%H:%M:%S')}, Accessed: {mem_item.access_count}, Last: {mem_item.last_accessed_timestamp.strftime('%H:%M:%S')}, Sources: {mem_item.source_turn_indices}\")\n",
    "        else:\n",
    "            print(\"  Mem0 memory store is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f421d",
   "metadata": {},
   "source": [
    "## 5. Execution and Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa59c0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comparative analysis of Raw vs. Mem0 approaches.\n",
      "\n",
      "STEP 1: Running Raw/Full-Context Approach...\n",
      "[Counters & Logs] All token counters and run logs have been reset.\n",
      "--- Running Raw/Full-Context Approach ---\n",
      "Raw Turn 1 User: Hi, let's start planning the 'New Marketing Campaign'. My primary goal is to inc...\n",
      "Raw Turn 1 Assistant: Great! Letâ€™s break this down into actionable steps to achieve your goal of incre...\n",
      "Raw Turn 2 User: For this campaign, the target audience is young adults aged 18-25....\n",
      "Raw Turn 2 Assistant: Got it! Targeting young adults aged 18-25 is a great demographic for increasing ...\n",
      "Raw Turn 3 User: I want to allocate a budget of $5000 for social media ads for the New Marketing ...\n",
      "Raw Turn 3 Assistant: With a $5000 budget for social media ads, we can create a focused and impactful ...\n",
      "Raw Turn 4 User: What's the main goal for the New Marketing Campaign?...\n",
      "Raw Turn 4 Assistant: The main goal for the **New Marketing Campaign** is to **increase brand awarenes...\n",
      "Raw Turn 5 User: Who are we targeting for this campaign?...\n",
      "Raw Turn 5 Assistant: For this campaign, weâ€™re targeting **young adults aged 18-25**. This demographic...\n",
      "Raw Turn 6 User: Let's also consider influencers. Add a task: 'Research potential influencers for...\n",
      "Raw Turn 6 Assistant: Absolutely! Leveraging influencers can be a powerful way to amplify your campaig...\n",
      "Raw Turn 7 User: Actually, let's increase the social media ad budget for the New Marketing Campai...\n",
      "Raw Turn 7 Assistant: Great! Increasing the social media ad budget to **$7500** gives us more flexibil...\n",
      "Raw Turn 8 User: What's the current budget for social media ads for the New Marketing Campaign?...\n",
      "Raw Turn 8 Assistant: The current budget for social media ads for the **New Marketing Campaign** is **...\n",
      "Raw Turn 9 User: What tasks do I have pending for this campaign?...\n",
      "Raw Turn 9 Assistant: Hereâ€™s a summary of the **pending tasks** for the **New Marketing Campaign** bas...\n",
      "Raw Turn 10 User: Also, for the New Marketing Campaign, I prefer visual content for this demograph...\n",
      "Raw Turn 10 Assistant: Got it! Visual content like **short videos** and **infographics** is a perfect f...\n",
      "\n",
      "--- Raw/Full-Context Approach Summary ---\n",
      "Total Prompt Tokens: 7492\n",
      "Total Completion Tokens: 1403\n",
      "Overall Total Tokens for Raw Approach: 8895\n",
      "Raw approach run complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution Block for Comparison ---\n",
    "print(\"Starting comparative analysis of Raw vs. Mem0 approaches.\\n\")\n",
    "\n",
    "# --- Run Raw/Full-Context Approach ---\n",
    "print(\"STEP 1: Running Raw/Full-Context Approach...\")\n",
    "\n",
    "# Reset all global token counters and logs to ensure a clean run\n",
    "reset_all_token_counters_and_logs()\n",
    "\n",
    "# Set verbosity for the raw approach (set to True for detailed logs)\n",
    "VERBOSE_RAW_RUN = False\n",
    "\n",
    "# Run the raw/full-context approach using the conversation script\n",
    "run_raw_full_context_approach(conversation_script)\n",
    "\n",
    "# Capture final token counts for the raw approach\n",
    "final_raw_prompt_tokens = total_prompt_tokens_raw\n",
    "final_raw_completion_tokens = total_completion_tokens_raw\n",
    "final_raw_total_tokens = final_raw_prompt_tokens + final_raw_completion_tokens\n",
    "\n",
    "print(\"Raw approach run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf30401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Running Mem0-Powered Conversational Approach...\n",
      "--- Running Mem0-Powered Conversational Approach ---\n",
      "\n",
      "--- Mem0 Turn 1/10 (statement) ---\n",
      "User: Hi, let's start planning the 'New Marketing Campaign'. My primary goal is to inc...\n",
      "[Extractor LLM] Parsed 2 fact(s).\n",
      "[MemoryOrchestrator] Extracted 2 fact(s).\n",
      "[MemoryOrchestrator] Fact 'The user wants to start planni...': LLM Decision -> ADD\n",
      "[MemoryOrchestrator] Fact 'The user's primary goal for th...': LLM Decision -> ADD\n",
      "Assistant (Ack): Okay, noted.\n",
      "\n",
      "--- Mem0 Turn 2/10 (statement) ---\n",
      "User: For this campaign, the target audience is young adults aged 18-25....\n",
      "[Extractor LLM] Parsed 1 fact(s).\n",
      "[MemoryOrchestrator] Extracted 1 fact(s).\n",
      "[MemoryOrchestrator] Fact 'The target audience for the 'N...': LLM Decision -> ADD\n",
      "Assistant (Ack): Okay, noted.\n",
      "\n",
      "--- Mem0 Turn 3/10 (statement) ---\n",
      "User: I want to allocate a budget of $5000 for social media ads for the New Marketing ...\n",
      "[Extractor LLM] Parsed 1 fact(s).\n",
      "[MemoryOrchestrator] Extracted 1 fact(s).\n",
      "[MemoryOrchestrator] Fact 'The user wants to allocate a b...': LLM Decision -> ADD\n",
      "Assistant (Ack): Okay, noted.\n",
      "\n",
      "--- Mem0 Turn 4/10 (query) ---\n",
      "User: What's the main goal for the New Marketing Campaign?...\n",
      "Assistant: The main goal for the New Marketing Campaign is to **increase brand awareness by...\n",
      "\n",
      "--- Mem0 Turn 5/10 (query) ---\n",
      "User: Who are we targeting for this campaign?...\n",
      "Assistant: The target audience for this campaign is **young adults aged 18-25**....\n",
      "\n",
      "--- Mem0 Turn 6/10 (statement) ---\n",
      "User: Let's also consider influencers. Add a task: 'Research potential influencers for...\n",
      "[Extractor LLM] Parsed 2 fact(s).\n",
      "[MemoryOrchestrator] Extracted 2 fact(s).\n",
      "[MemoryOrchestrator] Fact 'The user wants to consider inf...': LLM Decision -> ADD\n",
      "[MemoryOrchestrator] Fact 'The user added a task to resea...': LLM Decision -> UPDATE\n",
      "Assistant (Ack): Okay, noted.\n",
      "\n",
      "--- Mem0 Turn 7/10 (statement) ---\n",
      "User: Actually, let's increase the social media ad budget for the New Marketing Campai...\n",
      "[Extractor LLM] Parsed 1 fact(s).\n",
      "[MemoryOrchestrator] Extracted 1 fact(s).\n",
      "[MemoryOrchestrator] Fact 'The social media ad budget for...': LLM Decision -> UPDATE\n",
      "Assistant (Ack): Okay, noted.\n",
      "\n",
      "--- Mem0 Turn 8/10 (query) ---\n",
      "User: What's the current budget for social media ads for the New Marketing Campaign?...\n",
      "Assistant: The current budget for social media ads for the New Marketing Campaign is **$750...\n",
      "\n",
      "--- Mem0 Turn 9/10 (query) ---\n",
      "User: What tasks do I have pending for this campaign?...\n",
      "Assistant: Your pending tasks for the **New Marketing Campaign** are:  \n",
      "1. **Research poten...\n",
      "\n",
      "--- Mem0 Turn 10/10 (statement) ---\n",
      "User: Also, for the New Marketing Campaign, I prefer visual content for this demograph...\n",
      "[Extractor LLM] Parsed 1 fact(s).\n",
      "[MemoryOrchestrator] Extracted 1 fact(s).\n",
      "[MemoryOrchestrator] Fact 'The user prefers visual conten...': LLM Decision -> ADD\n",
      "Assistant (Ack): Okay, noted.\n",
      "\n",
      "--- Mem0-Powered Conversational Approach Summary (Global Counters) ---\n",
      "Mem0 approach run complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Run Mem0-Powered Approach ---\n",
    "print(\"STEP 2: Running Mem0-Powered Conversational Approach...\")\n",
    "\n",
    "# NOTE: If running this cell independently, ensure Mem0 counters and logs are reset for a clean run.\n",
    "# The counters are reset in reset_all_token_counters_and_logs() if called previously.\n",
    "# If you want to run only the Mem0 part, uncomment the next line:\n",
    "# reset_all_token_counters_and_logs()\n",
    "\n",
    "# Clear previous Mem0 run log if any (raw_run_log is not reset here)\n",
    "global mem0_run_log\n",
    "mem0_run_log = []\n",
    "\n",
    "# Set verbosity for Mem0 run and memory store operations\n",
    "VERBOSE_MEM0_RUN = False  # Set to True for detailed Mem0 run logs\n",
    "mem0_memory_store.verbose = VERBOSE_MEM0_RUN\n",
    "\n",
    "# Run the Mem0-powered conversational approach using the conversation script\n",
    "run_mem0_approach_conversation(conversation_script, mem0_memory_store)\n",
    "\n",
    "# Collect final Mem0 token counts for each sub-task and overall\n",
    "final_mem0_conv_prompt_tokens = total_prompt_tokens_mem0_conversation\n",
    "final_mem0_conv_completion_tokens = total_completion_tokens_mem0_conversation\n",
    "final_mem0_extr_prompt_tokens = total_prompt_tokens_mem0_extraction\n",
    "final_mem0_extr_completion_tokens = total_completion_tokens_mem0_extraction\n",
    "final_mem0_upd_prompt_tokens = total_prompt_tokens_mem0_update\n",
    "final_mem0_upd_completion_tokens = total_completion_tokens_mem0_update\n",
    "\n",
    "# Calculate overall Mem0 prompt, completion, and total tokens\n",
    "final_mem0_overall_prompt_tokens = (\n",
    "    final_mem0_conv_prompt_tokens + final_mem0_extr_prompt_tokens + final_mem0_upd_prompt_tokens\n",
    ")\n",
    "final_mem0_overall_completion_tokens = (\n",
    "    final_mem0_conv_completion_tokens + final_mem0_extr_completion_tokens + final_mem0_upd_completion_tokens\n",
    ")\n",
    "final_mem0_overall_total_tokens = (\n",
    "    final_mem0_overall_prompt_tokens + final_mem0_overall_completion_tokens\n",
    ")\n",
    "\n",
    "print(\"Mem0 approach run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1f44723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data dictionary for comparative analysis DataFrame\n",
    "# Each key is a column: 'Metric', 'Raw Approach', 'Mem0 Approach'\n",
    "data = {\n",
    "    'Metric': [\n",
    "        'Prompt Tokens',                  # Total prompt tokens used (Raw vs Mem0 overall)\n",
    "        'Completion Tokens',              # Total completion tokens used (Raw vs Mem0 overall)\n",
    "        'Total Tokens',                   # Total tokens (Prompt + Completion)\n",
    "        '',                               # Separator row for readability\n",
    "        'Mem0: Conversational Prompt',    # Mem0: Prompt tokens for conversational (query) turns\n",
    "        'Mem0: Conversational Completion',# Mem0: Completion tokens for conversational (query) turns\n",
    "        'Mem0: Extraction Prompt',        # Mem0: Prompt tokens for extraction sub-task\n",
    "        'Mem0: Extraction Completion',    # Mem0: Completion tokens for extraction sub-task\n",
    "        'Mem0: Update Logic Prompt',      # Mem0: Prompt tokens for update logic sub-task\n",
    "        'Mem0: Update Logic Completion'   # Mem0: Completion tokens for update logic sub-task\n",
    "    ],\n",
    "    'Raw Approach': [\n",
    "        final_raw_prompt_tokens,          # Raw approach: total prompt tokens\n",
    "        final_raw_completion_tokens,      # Raw approach: total completion tokens\n",
    "        final_raw_total_tokens,           # Raw approach: total tokens\n",
    "        '',                              # Separator (blank)\n",
    "        '-',                             # Not applicable for Raw approach\n",
    "        '-',                             # Not applicable for Raw approach\n",
    "        '-',                             # Not applicable for Raw approach\n",
    "        '-',                             # Not applicable for Raw approach\n",
    "        '-',                             # Not applicable for Raw approach\n",
    "        '-'                              # Not applicable for Raw approach\n",
    "    ],\n",
    "    'Mem0 Approach': [\n",
    "        final_mem0_overall_prompt_tokens,     # Mem0: overall prompt tokens (sum of all sub-tasks)\n",
    "        final_mem0_overall_completion_tokens, # Mem0: overall completion tokens (sum of all sub-tasks)\n",
    "        final_mem0_overall_total_tokens,      # Mem0: overall total tokens\n",
    "        '',                                   # Separator (blank)\n",
    "        final_mem0_conv_prompt_tokens,        # Mem0: conversational prompt tokens\n",
    "        final_mem0_conv_completion_tokens,    # Mem0: conversational completion tokens\n",
    "        final_mem0_extr_prompt_tokens,        # Mem0: extraction prompt tokens\n",
    "        final_mem0_extr_completion_tokens,    # Mem0: extraction completion tokens\n",
    "        final_mem0_upd_prompt_tokens,         # Mem0: update logic prompt tokens\n",
    "        final_mem0_upd_completion_tokens      # Mem0: update logic completion tokens\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c13bbad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analysis --- \n",
      "Mem0 was more token efficient, saving 3163 tokens (35.56%) compared to Raw.\n",
      "\n",
      "Key benefit of Mem0: Token efficiency in *longer* conversations due to stable query prompt sizes.\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for comparative analysis using the prepared data dictionary\n",
    "comparison_df = pd.DataFrame(data)\n",
    "\n",
    "# Print analysis summary based on token usage\n",
    "print(\"\\n--- Analysis --- \")\n",
    "\n",
    "# Case 1: Both approaches have zero tokens (likely an error or incomplete run)\n",
    "if final_raw_total_tokens == 0 and final_mem0_overall_total_tokens == 0:\n",
    "    print(\"Token counts are both zero. Ensure runs completed successfully and API calls were made.\")\n",
    "\n",
    "# Case 2: Mem0 approach used fewer tokens than Raw (token savings)\n",
    "elif final_mem0_overall_total_tokens < final_raw_total_tokens:\n",
    "    savings = final_raw_total_tokens - final_mem0_overall_total_tokens\n",
    "    percentage_savings = (savings / final_raw_total_tokens) * 100 if final_raw_total_tokens > 0 else 0\n",
    "    print(f\"Mem0 was more token efficient, saving {savings} tokens ({percentage_savings:.2f}%) compared to Raw.\")\n",
    "\n",
    "# Case 3: Raw approach used fewer tokens than Mem0 (token overhead for Mem0)\n",
    "elif final_raw_total_tokens < final_mem0_overall_total_tokens:\n",
    "    overhead = final_mem0_overall_total_tokens - final_raw_total_tokens\n",
    "    percentage_overhead = (overhead / final_raw_total_tokens) * 100 if final_raw_total_tokens > 0 else float('inf')\n",
    "    print(f\"Raw was more token efficient. Mem0 had an overhead of {overhead} tokens ({percentage_overhead:.2f}%).\")\n",
    "    print(\"(This can be expected for short conversations due to Mem0's extraction/update costs.)\")\n",
    "\n",
    "# Case 4: Both approaches used approximately the same number of tokens\n",
    "else:\n",
    "    print(f\"Both approaches used approx. the same tokens: {final_mem0_overall_total_tokens}.\")\n",
    "\n",
    "# General note about Mem0's advantage in longer conversations\n",
    "print(\"\\nKey benefit of Mem0: Token efficiency in *longer* conversations due to stable query prompt sizes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "477b8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'Percentage Difference' to the comparison DataFrame.\n",
    "# For each row, if both 'Raw Approach' and 'Mem0 Approach' are integers and 'Raw Approach' is not zero,\n",
    "# calculate the percentage difference as: \n",
    "#   -((Mem0 Approach - Raw Approach) / Raw Approach) * 100\n",
    "# This shows the percent token savings (positive means Mem0 used fewer tokens).\n",
    "# If not applicable, set as None.\n",
    "comparison_df['Percentage Difference'] = comparison_df.apply(\n",
    "    lambda row: (\n",
    "        -(row['Mem0 Approach'] - row['Raw Approach']) / row['Raw Approach'] * 100\n",
    "        if isinstance(row['Raw Approach'], int) and isinstance(row['Mem0 Approach'], int) and row['Raw Approach'] != 0\n",
    "        else None\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f74b500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Raw Approach</th>\n",
       "      <th>Mem0 Approach</th>\n",
       "      <th>Percentage Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prompt Tokens</td>\n",
       "      <td>7492</td>\n",
       "      <td>5244</td>\n",
       "      <td>30.005339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Completion Tokens</td>\n",
       "      <td>1403</td>\n",
       "      <td>488</td>\n",
       "      <td>65.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total Tokens</td>\n",
       "      <td>8895</td>\n",
       "      <td>5732</td>\n",
       "      <td>35.559303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mem0: Conversational Prompt</td>\n",
       "      <td>-</td>\n",
       "      <td>814</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mem0: Conversational Completion</td>\n",
       "      <td>-</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mem0: Extraction Prompt</td>\n",
       "      <td>-</td>\n",
       "      <td>1522</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mem0: Extraction Completion</td>\n",
       "      <td>-</td>\n",
       "      <td>173</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mem0: Update Logic Prompt</td>\n",
       "      <td>-</td>\n",
       "      <td>2908</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mem0: Update Logic Completion</td>\n",
       "      <td>-</td>\n",
       "      <td>214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Metric Raw Approach Mem0 Approach  \\\n",
       "0                    Prompt Tokens         7492          5244   \n",
       "1                Completion Tokens         1403           488   \n",
       "2                     Total Tokens         8895          5732   \n",
       "3                                                               \n",
       "4      Mem0: Conversational Prompt            -           814   \n",
       "5  Mem0: Conversational Completion            -           101   \n",
       "6          Mem0: Extraction Prompt            -          1522   \n",
       "7      Mem0: Extraction Completion            -           173   \n",
       "8        Mem0: Update Logic Prompt            -          2908   \n",
       "9    Mem0: Update Logic Completion            -           214   \n",
       "\n",
       "   Percentage Difference  \n",
       "0              30.005339  \n",
       "1              65.217391  \n",
       "2              35.559303  \n",
       "3                    NaN  \n",
       "4                    NaN  \n",
       "5                    NaN  \n",
       "6                    NaN  \n",
       "7                    NaN  \n",
       "8                    NaN  \n",
       "9                    NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f70492",
   "metadata": {},
   "source": [
    "## 6. Discussion of Expected Results and Further Improvements\n",
    "\n",
    "After running the notebook with a capable LLM:\n",
    "\n",
    "**Token Efficiency:**\n",
    "*   For short conversations, Mem0's extraction/update overhead might make its total token count comparable or slightly higher than Raw. However, for longer conversations, Mem0 should become significantly more token-efficient as its query prompt sizes (query + K retrieved memories) remain stable, while Raw's prompt (full history) grows linearly.\n",
    "\n",
    "**Response Quality & Context Handling:**\n",
    "*   **Raw Approach:** Can be good with capable LLMs but risks \"lost in the middle\" issues, recency bias, and difficulty with conflicting/updated info in very long contexts.\n",
    "*   **Mem0 Approach:** Aims for focused, accurate responses using relevant retrieved memories. The `UPDATE` mechanism is key for handling evolving information. Quality hinges on accurate extraction and update logic.\n",
    "\n",
    "**Challenges & Improvements for this Notebook's Mem0:**\n",
    "*   **LLM Dependency:** Mem0's internal operations (extraction, update decisions) are highly sensitive to the `LLM_MODEL`'s capabilities.\n",
    "*   **Prompt Engineering:** Prompts for extraction and update are crucial and can always be refined.\n",
    "*   **Error Handling:** More robust parsing of LLM JSON outputs and error recovery.\n",
    "*   **Full Mem0 Features:** Implementing `DELETE`, conversation summary `S`, and graph memory (`Mem0g`) would align closer with the paper.\n",
    "\n",
    "This notebook serves as a foundational exploration. Production-ready systems require more engineering and potentially fine-tuned models for memory sub-tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-mimind-thinking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
